{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b43134f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# https://stackoverflow.com/questions/34478398/import-local-function-\n",
    "# from-a-module-housed-in-another-directory-with-relative-im\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src import local_module as local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fc39ff",
   "metadata": {},
   "source": [
    "# Data Caveats\n",
    "\n",
    "- The geographic fatality data will not total the tabular fatality data because some of the tabular records do not have valid geographic information.\n",
    "- Some of the fatality points lie on the borders of multiple census tracts, and are assigned to a single tract based on programmatic rules in the pandas library.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf19c39",
   "metadata": {},
   "source": [
    "# Geographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3ca10da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/gds/lib/python3.9/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www2.census.gov'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m dfs \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# initialize empty list to hold tract geodataframes\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m zips:\n\u001b[0;32m---> 13\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mgp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# read in tract files and add to list\u001b[39;00m\n\u001b[1;32m     14\u001b[0m tracts \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dfs) \u001b[38;5;66;03m# combine tracts into single geodataframe\u001b[39;00m\n\u001b[1;32m     15\u001b[0m tracts\u001b[38;5;241m.\u001b[39mto_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/raw/tracts.geojson\u001b[39m\u001b[38;5;124m\"\u001b[39m, driver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeoJSON\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gds/lib/python3.9/site-packages/geopandas/io/file.py:242\u001b[0m, in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m from_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_url(filename):\n\u001b[0;32m--> 242\u001b[0m     req \u001b[38;5;241m=\u001b[39m \u001b[43m_urlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m     path_or_bytes \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    244\u001b[0m     from_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gds/lib/python3.9/urllib/request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gds/lib/python3.9/urllib/request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gds/lib/python3.9/urllib/request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gds/lib/python3.9/urllib/request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    560\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gds/lib/python3.9/urllib/request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gds/lib/python3.9/urllib/request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "# citation: https://andrewpwheeler.com/2022/02/28/downloading-geo-files-from-census-ftp-using-python/\n",
    "def get_zip(url):\n",
    "    webpage = requests.get(url, verify=False)\n",
    "    soup = BeautifulSoup(webpage.content,\"html.parser\")\n",
    "    zip_files = soup.find_all(\"a\", href=re.compile(r\"zip\")) # find anchor elements on page whose href contain \"zip\"\n",
    "    zip_urls = [os.path.join(url, i[\"href\"]) for i in zip_files] # join the webpage url to the file name for each file\n",
    "    return zip_urls\n",
    "\n",
    "url = r\"https://www2.census.gov/geo/tiger/TIGER2021/TRACT/\"\n",
    "zips = get_zip(url)\n",
    "dfs = [] # initialize empty list to hold tract geodataframes\n",
    "for file in zips:\n",
    "    dfs.append(gp.read_file(file)) # read in tract files and add to list\n",
    "tracts = pd.concat(dfs) # combine tracts into single geodataframe\n",
    "tracts.to_file(\"../data/raw/tracts.geojson\", driver=\"GeoJSON\") # export to local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835cc34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4506abb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracts.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9761efd",
   "metadata": {},
   "source": [
    "# FARS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e775010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_cols = [\n",
    "    \"ST_CASE\",\n",
    "    \"VEH_NO\",\n",
    "    \"PER_NO\", # p. 248 of 2022 User Manual PDF\n",
    "    \"PER_TYP\", # p.265 of 2022 User Manual PDF\n",
    "    # \"PER_TYPNAME\", # p.265 of 2022 User Manual PDF\n",
    "    \"INJ_SEV\", # p.267 of 2022 User Manual PDF\n",
    "    \"INJ_SEVNAME\" # p.267 of 2022 User Manual PDF\n",
    "]\n",
    "\n",
    "accident_cols = [\n",
    "    \"ST_CASE\",\n",
    "    \"LATITUDE\", # p.67 of User Manual PDF\n",
    "    \"LONGITUD\" # p.68 of User Manual PDF\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace2a3c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# citation: https://stackoverflow.com/questions/44575251/reading-multiple-files-contained-in-a-zip-file-with-pandas\n",
    "# this should work but doesn't\n",
    "\n",
    "zip_file = ZipFile('../data/raw/FARS2020NationalCSV.zip')\n",
    "\n",
    "# https://docs.python.org/3/library/codecs.html#error-handlers\n",
    "# both files ran into decoding issues\n",
    "accidents = pd.read_csv(zip_file.open(\"accident.CSV\"), usecols=accident_cols, encoding_errors=\"surrogateescape\")\n",
    "person = pd.read_csv(zip_file.open(\"person.csv\"), usecols=person_cols, encoding_errors=\"surrogateescape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e836fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "person.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e84f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67718a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join spatial data from accidents to person data\n",
    "fars = person.merge(accidents, how=\"left\", on=\"ST_CASE\", validate=\"m:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93cb924",
   "metadata": {},
   "outputs": [],
   "source": [
    "fars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baeec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use lat/lon to create geometry\n",
    "fars = gp.GeoDataFrame(fars, geometry=gp.points_from_xy(fars.LONGITUD, fars.LATITUDE), crs=4269)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b4e13e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fars.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7865ac5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# filter records to only fatal injuries\n",
    "fars = fars.loc[fars[\"INJ_SEV\"]==4]\n",
    "fars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3d950e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# lat/lon values signifiying not reported, unavailable, or reported as unknown\n",
    "lat_vals = [77.7777000, 88.8888000, 99.9999000]\n",
    "lon_vals = [777.7777000, 888.8888000, 999.9999000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baefb822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see which records have invalid location information\n",
    "invalid_loc = fars.loc[fars[\"LATITUDE\"].isin(lat_vals) | fars[\"LONGITUD\"].isin(lon_vals)]\n",
    "invalid_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0f9ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove records with invalid location info\n",
    "fars = fars[~fars.isin(invalid_loc)].dropna(how = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f3127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fars.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b84720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remap values in the PER_TYP column \n",
    "fars = local.remap_PER_TYP(fars, 2020)\n",
    "\n",
    "# dict mapping multiple per_typs to one\n",
    "combine_dict = {\n",
    "    \"Other non-occupant\": 'Other/unknown non-occupant', \n",
    "    \"Unknown non-occupant type\": 'Other/unknown non-occupant', \n",
    "    \"Unknown person type\": 'Other/unknown non-occupant'\n",
    "    }\n",
    "\n",
    "fars.replace(to_replace=combine_dict, inplace=True) # combine categories in per_typ column\n",
    "fars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e05db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fars.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce956df",
   "metadata": {},
   "source": [
    "# Joining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b17f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_count(points, polygons, groupby_col, count_col, unique_id):\n",
    "    \"\"\"\n",
    "    This function counts the number of points in each polygon. If\n",
    "    points lie on the boundaries of multiple polygons, the function\n",
    "    assigns them based on the default args of pandas.drop_duplicates()\n",
    "    \n",
    "    points: geodataframe of points\n",
    "    polygons: geodataframe of polygons\n",
    "    groupby_col: the unique ID of polygons\n",
    "    count_col: the name assigned to the count column in the output\n",
    "    unique_id: the unique ID (string or list-like) of points\n",
    "    \"\"\"\n",
    "    \n",
    "    # spatial join polygon attributes to the points\n",
    "    join = points.sjoin(polygons, how=\"left\", predicate=\"intersects\")\n",
    "    \n",
    "    # find and drop duplicates based on unique ID field(s)\n",
    "    # any duplicates are likely the result of points on the boundary \n",
    "    # between two polygons\n",
    "    join.drop_duplicates(subset=unique_id, inplace=True)  \n",
    "    \n",
    "    # group points by specified column\n",
    "    grouped = join.groupby(by=groupby_col)\n",
    "    \n",
    "    # count the number of points in each group, and convert to dataframe\n",
    "    counts = grouped.size().reset_index()\n",
    "    \n",
    "    # rename the count column to specified value\n",
    "    counts.rename(columns={0: count_col}, inplace=True)\n",
    "    \n",
    "    # merge count column to polygons, maintaining polygons with no count\n",
    "    polygons = polygons.merge(counts, on=groupby_col, how=\"left\")\n",
    "    \n",
    "    # replace NaN in count column with 0\n",
    "    polygons.fillna(value={count_col: 0}, inplace=True)\n",
    "    \n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c6264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total 2022 fatalities per census tract\n",
    "tracts = join_count(fars, tracts, \"GEOID\", \"TOT_FATALITIES\", [\"ST_CASE\", \"VEH_NO\", \"PER_NO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c45c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Pedestrian', \n",
    "              'Bicyclist',\n",
    "              'Other/unknown non-occupant'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c1fdfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    # subset category of interest\n",
    "    subset = fars[fars[\"PER_TYP\"] == category]\n",
    "    \n",
    "    # count the number of fatalities for the subset, and add as new attribute   \n",
    "    tracts = join_count(subset, tracts, \"GEOID\", category.upper() + \"_FATALITIES\", [\"ST_CASE\", \"VEH_NO\", \"PER_NO\"])\n",
    "    \n",
    "    # calculate percentage of total fatalities in census tract as new attribute\n",
    "    tracts[\"P_\" + category.upper() + \"_FATALITIES\"] = tracts[category.upper() + \"_FATALITIES\"] / tracts[\"TOT_FATALITIES\"]\n",
    "    \n",
    "    # quick check to see max fatalities in a census tract\n",
    "    print(tracts[category.upper() + \"_FATALITIES\"].max())\n",
    "    \n",
    "# tracts = tracts.loc[:,\"STATEFP\": \"TOT_FATALITIES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df1aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN with 0 in percentage columns\n",
    "tracts.fillna(\n",
    "    {\n",
    "    \"P_PEDESTRIAN_FATALITIES\": 0, \n",
    "    \"P_BICYCLIST_FATALITIES\": 0, \n",
    "    \"P_OTHER/UNKNOWN NON-OCCUPANT_FATALITIES\": 0\n",
    "    }, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f14c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc065218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "# https://code.activestate.com/recipes/577775-state-fips-codes-dict/\n",
    "state_codes = {\n",
    "    'WA': '53', \n",
    "    'DE': '10', \n",
    "    'DC': '11', \n",
    "    'WI': '55', \n",
    "    'WV': '54', \n",
    "    # 'HI': '15',\n",
    "    'FL': '12', \n",
    "    'WY': '56', \n",
    "    # 'PR': '72', \n",
    "    'NJ': '34',\n",
    "    'NM': '35', \n",
    "    'TX': '48',\n",
    "    'LA': '22', \n",
    "    'NC': '37', \n",
    "    'ND': '38', \n",
    "    'NE': '31', \n",
    "    'TN': '47', \n",
    "    'NY': '36',\n",
    "    'PA': '42', \n",
    "    # 'AK': '02', \n",
    "    'NV': '32', \n",
    "    'NH': '33', \n",
    "    'VA': '51', \n",
    "    'CO': '08',\n",
    "    'CA': '06', \n",
    "    'AL': '01', \n",
    "    'AR': '05', \n",
    "    'VT': '50', \n",
    "    'IL': '17', \n",
    "    'GA': '13',\n",
    "    'IN': '18', \n",
    "    'IA': '19', \n",
    "    'MA': '25', \n",
    "    'AZ': '04', \n",
    "    'ID': '16', \n",
    "    'CT': '09',\n",
    "    'ME': '23', \n",
    "    'MD': '24', \n",
    "    'OK': '40', \n",
    "    'OH': '39', \n",
    "    'UT': '49', \n",
    "    'MO': '29',\n",
    "    'MN': '27', \n",
    "    'MI': '26', \n",
    "    'RI': '44', \n",
    "    'KS': '20', \n",
    "    'MT': '30', \n",
    "    'MS': '28',\n",
    "    'SC': '45', \n",
    "    'KY': '21', \n",
    "    'OR': '41', \n",
    "    'SD': '46'\n",
    "}\n",
    "state_codes = list(state_codes.values())\n",
    "states = gp.read_file(\"https://www2.census.gov/geo/tiger/TIGER2021/STATE/tl_2021_us_state.zip\")\n",
    "states = states.loc[states[\"STATEFP\"].isin(state_codes)] # cut to lower 48\n",
    "tracts_viz = states.overlay(tracts, how='intersection')\n",
    "tracts_viz.to_crs(epsg=6350, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7026e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(15, 10))\n",
    "\n",
    "tracts_viz.plot(\n",
    "    column=\"P_PEDESTRIAN_FATALITIES\", \n",
    "    linewidth=0,\n",
    "    legend=True,\n",
    "    legend_kwds={\"shrink\":0.5},\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_axis_off()\n",
    "ax.set_title(\"Percentage of Pedestrian Fatalities by Census Tract, 2020\", size=15)\n",
    "f.set_facecolor('0.9')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f277fb",
   "metadata": {},
   "source": [
    "# Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ebb50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-motorist fatalities by census tract for a single year\n",
    "tracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961527de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracts[\"TOT_FATALITIES\"].sum()\n",
    "# tracts[\"PEDESTRIAN_FATALITIES\"].sum()\n",
    "# tracts[\"BICYCLIST_FATALITIES\"].sum()\n",
    "# tracts[\"PERSONAL CONVEYANCE_FATALITIES\"].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
